{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikcreebYvqa4",
        "outputId": "0944245b-ac3b-45ef-a7e7-32020baceb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "jVt7kJpZvv9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/Univerui/0 MAGISTRAS/tiriamasis/embeddings/'\n",
        "\n",
        "bert_data = pd.read_csv(DATA_PATH+'bert_embeddings.csv')\n",
        "labse_data = pd.read_csv(DATA_PATH+'labse_embeddings.csv')\n",
        "w2v_data = pd.read_csv(DATA_PATH+'w2v_embeddings.csv')"
      ],
      "metadata": {
        "id": "BQt5S8yev53M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClusteringLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_clusters, encoding_dim, alpha, **kwargs):\n",
        "    super(ClusteringLayer, self).__init__(**kwargs)\n",
        "    self.n_clusters = n_clusters\n",
        "    self.clusters = tf.Variable(initial_value=tf.zeros([n_clusters, encoding_dim]),\n",
        "                                trainable=True)\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    q = 1. / (1. + (tf.reduce_sum(tf.square(tf.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "    q **= (self.alpha + 1.) / 2.\n",
        "    q = tf.transpose(tf.transpose(q) / tf.reduce_sum(q, axis=1))\n",
        "    return q"
      ],
      "metadata": {
        "id": "7v12ODTWyJSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder_model(input_dim, latent_dim):\n",
        "  input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
        "  encoder = tf.keras.layers.Dense(int(input_dim/2.), activation='relu')(input_layer)\n",
        "  encoder = tf.keras.layers.Dense(int(input_dim/4.), activation='relu')(encoder)\n",
        "  encoder = tf.keras.layers.Dense(int(input_dim/8.), activation='relu')(encoder)\n",
        "\n",
        "  encoder_output = tf.keras.layers.Dense(latent_dim, activation='relu')(encoder)\n",
        "\n",
        "  decoder = tf.keras.layers.Dense(int(input_dim/8.), activation='relu')(encoder_output)\n",
        "  decoder = tf.keras.layers.Dense(int(input_dim/4.), activation='relu')(decoder)\n",
        "  decoder = tf.keras.layers.Dense(int(input_dim/2.), activation='relu')(decoder)\n",
        "\n",
        "  decoder_output = tf.keras.layers.Dense(input_dim, activation='relu')(decoder)\n",
        "\n",
        "  autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder_output)\n",
        "  encoder = tf.keras.models.Model(inputs=input_layer, outputs=encoder_output)\n",
        "\n",
        "  return autoencoder, encoder\n",
        "\n",
        "def target_distribution(q):\n",
        "  weight = q ** 2 / tf.reduce_sum(q, axis=0)\n",
        "  return tf.transpose(tf.transpose(weight) / tf.reduce_sum(weight, axis=1))"
      ],
      "metadata": {
        "id": "JIOJ_7Ia26s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DEC(data, num_clusters, name, encoding_dim=16, batch_size=256, maxiter=10000,\n",
        "        update_interval=140, threshold=0.001, alpha=1.):\n",
        "  scaler = StandardScaler()\n",
        "  x_data = scaler.fit_transform(data)\n",
        "\n",
        "  input_dim = x_data.shape[1]\n",
        "\n",
        "  # AutoEncoder\n",
        "  autoencoder, encoder = autoencoder_model(input_dim, encoding_dim)\n",
        "  autoencoder.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
        "  autoencoder.fit(x_data, x_data, epochs=50, batch_size=batch_size, verbose=0)\n",
        "\n",
        "  # Clustering centers\n",
        "  cluster_model = KMeans(n_clusters=num_clusters, n_init='auto')\n",
        "  y_pred = cluster_model.fit_predict(encoder.predict(x_data))\n",
        "  cluster_centers = cluster_model.cluster_centers_\n",
        "\n",
        "  # Clustering layers\n",
        "  clustering_layer = ClusteringLayer(num_clusters, encoding_dim, alpha, name='clustering')(encoder.output)\n",
        "  model = tf.keras.models.Model(inputs=encoder.input, outputs=clustering_layer)\n",
        "  model.get_layer(name='clustering').clusters.assign(cluster_centers)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(), loss='kld')\n",
        "\n",
        "  index_array = np.arange(x_data.shape[0])\n",
        "\n",
        "  # Training\n",
        "  for i in range(maxiter):\n",
        "    if i % update_interval == 0:\n",
        "      q = model.predict(x_data, verbose=0)\n",
        "      p = target_distribution(q)\n",
        "      y_pred = q.argmax(axis=1)\n",
        "      y_proba = q\n",
        "\n",
        "      if i > 0 and np.sum(y_pred != y_pred_last) / y_pred.shape[0] <  threshold:\n",
        "        break\n",
        "      y_pred_last = y_pred\n",
        "\n",
        "    idx = index_array[i * batch_size: min((i+1) * batch_size, x_data.shape[0])]\n",
        "    loss = model.train_on_batch(tf.gather(x_data, idx), tf.gather(p, idx))\n",
        "\n",
        "    if i % update_interval == 0:\n",
        "      print(f\"Iteration: {i}: loss = {loss}\")\n",
        "\n",
        "  encoder.save(f'/content/drive/MyDrive/Univerui/0 MAGISTRAS/tiriamasis/clustering_outputs/DEC/{name}_encoder.h5')\n",
        "  model.save(f'/content/drive/MyDrive/Univerui/0 MAGISTRAS/tiriamasis/clustering_outputs/DEC/{name}_clustering_model.h5')\n",
        "\n",
        "  return y_pred, y_proba"
      ],
      "metadata": {
        "id": "cNQdncKDw9F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_pred, bert_proba = DEC(bert_data, 20, 'bert', batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DZqrGIo7nWk",
        "outputId": "669993a0-ab88-481b-ea05-bc433dc9a0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "Iteration: 0: loss = 0.1215776577591896\n",
            "Iteration: 140: loss = 0.06486725062131882\n",
            "Iteration: 280: loss = 0.1424783617258072\n",
            "Iteration: 420: loss = 0.19619962573051453\n",
            "Iteration: 560: loss = 0.2278680056333542\n",
            "Iteration: 700: loss = 0.25229695439338684\n",
            "Iteration: 840: loss = 0.2674711048603058\n",
            "Iteration: 980: loss = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClusteringLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_clusters, encoding_dim, alpha, **kwargs):\n",
        "    super(ClusteringLayer, self).__init__(**kwargs)\n",
        "    self.n_clusters = n_clusters\n",
        "    self.clusters = tf.Variable(initial_value=tf.zeros([n_clusters, encoding_dim]),\n",
        "                                trainable=True)\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    q = 1. / (1. + (tf.reduce_sum(tf.square(tf.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "    q **= (self.alpha + 1.) / 2.\n",
        "    q = tf.transpose(tf.transpose(q) / tf.reduce_sum(q, axis=1))\n",
        "    return q\n",
        "\n",
        "def autoencoder_model(input_dim, latent_dim):\n",
        "  input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
        "  encoder = tf.keras.layers.Dense(int(input_dim/2.), activation='relu')(input_layer)\n",
        "  encoder = tf.keras.layers.Dense(int(input_dim/4.), activation='relu')(encoder)\n",
        "  encoder = tf.keras.layers.Dense(int(input_dim/8.), activation='relu')(encoder)\n",
        "\n",
        "  encoder_output = tf.keras.layers.Dense(latent_dim, activation='relu')(encoder)\n",
        "\n",
        "  decoder = tf.keras.layers.Dense(int(input_dim/8.), activation='relu')(encoder_output)\n",
        "  decoder = tf.keras.layers.Dense(int(input_dim/4.), activation='relu')(decoder)\n",
        "  decoder = tf.keras.layers.Dense(int(input_dim/2.), activation='relu')(decoder)\n",
        "\n",
        "  decoder_output = tf.keras.layers.Dense(input_dim, activation='relu')(decoder)\n",
        "\n",
        "  autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder_output)\n",
        "  encoder = tf.keras.models.Model(inputs=input_layer, outputs=encoder_output)\n",
        "\n",
        "  return autoencoder, encoder\n",
        "\n",
        "def target_distribution(q):\n",
        "  weight = q ** 2 / (tf.reduce_sum(q, axis=0) + tf.keras.backend.epsilon())\n",
        "  return tf.transpose(tf.transpose(weight) / (tf.reduce_sum(weight, axis=1)+ tf.keras.backend.epsilon()))\n",
        "\n",
        "def DEC(data, num_clusters, name, encoding_dim=16, learning_rate=0.001, batch_size=256, maxiter=10000,\n",
        "        update_interval=140, threshold=0.001, alpha=1.):\n",
        "  scaler = StandardScaler()\n",
        "  x_data = scaler.fit_transform(data)\n",
        "\n",
        "  input_dim = x_data.shape[1]\n",
        "\n",
        "  # AutoEncoder\n",
        "  autoencoder, encoder = autoencoder_model(input_dim, encoding_dim)\n",
        "  autoencoder.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
        "  autoencoder.fit(x_data, x_data, epochs=50, batch_size=batch_size, verbose=0)\n",
        "\n",
        "  # Clustering centers\n",
        "  cluster_model = KMeans(n_clusters=num_clusters, n_init='auto')\n",
        "  y_pred = cluster_model.fit_predict(encoder.predict(x_data, verbose=0))\n",
        "  cluster_centers = cluster_model.cluster_centers_\n",
        "\n",
        "  # Clustering layers\n",
        "  clustering_layer = ClusteringLayer(num_clusters, encoding_dim, alpha, name='clustering')(encoder.output)\n",
        "  model = tf.keras.models.Model(inputs=encoder.input, outputs=clustering_layer)\n",
        "  model.get_layer(name='clustering').clusters.assign(cluster_centers)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
        "                                                   clipvalue=1.),\n",
        "                loss='kld')\n",
        "\n",
        "  index_array = np.arange(x_data.shape[0])\n",
        "\n",
        "  # Training\n",
        "  for i in range(maxiter):\n",
        "    if i % update_interval == 0:\n",
        "      q = model.predict(x_data, verbose=0)\n",
        "      p = target_distribution(q)\n",
        "      y_pred = q.argmax(axis=1)\n",
        "      y_proba = q\n",
        "\n",
        "      if i > 0 and np.sum(y_pred != y_pred_last) / y_pred.shape[0] <  threshold:\n",
        "        break\n",
        "      y_pred_last = y_pred\n",
        "\n",
        "    idx = index_array[i * batch_size: min((i+1) * batch_size, x_data.shape[0])]\n",
        "    loss = model.train_on_batch(tf.gather(x_data, idx), tf.gather(p, idx))\n",
        "\n",
        "    if i % update_interval == 0:\n",
        "      print(f\"Iteration: {i}: loss = {loss}\")\n",
        "\n",
        "  encoder.save(f'/content/drive/MyDrive/Univerui/0 MAGISTRAS/tiriamasis/clustering_outputs/DEC/{name}_encoder.h5')\n",
        "  model.save(f'/content/drive/MyDrive/Univerui/0 MAGISTRAS/tiriamasis/clustering_outputs/DEC/{name}_clustering_model.h5')\n",
        "\n",
        "  return y_pred, y_proba"
      ],
      "metadata": {
        "id": "X-V0jJOVOtQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_pred, bert_proba = DEC(bert_data, 20, 'bert', encoding_dim=64, batch_size=32, learning_rate=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BE2yGh-WiK2",
        "outputId": "5f34aa2f-5130-46cd-d3f7-340f76a4a568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0: loss = 0.057842668145895004\n",
            "Iteration: 140: loss = 0.03253987058997154\n",
            "Iteration: 280: loss = 0.04777824506163597\n",
            "Iteration: 420: loss = 0.071541927754879\n",
            "Iteration: 560: loss = 0.09459365159273148\n",
            "Iteration: 700: loss = 0.11490293592214584\n",
            "Iteration: 840: loss = 0.1309777945280075\n",
            "Iteration: 980: loss = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_pred[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXBJQ9gMW1U8",
        "outputId": "ac49d89a-1149-4e77-de3f-b6c1e281673d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 11,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_proba[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibUZ3yjd10l",
        "outputId": "e9b5b3c2-605c-4d8d-b557-c99ef58b2ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03422335, 0.02310062, 0.27634946, 0.03301444, 0.03332696,\n",
              "       0.03662706, 0.0267931 , 0.02875627, 0.02862794, 0.04006774,\n",
              "       0.01736554, 0.09502963, 0.02680646, 0.06378695, 0.06004831,\n",
              "       0.0657844 , 0.03439815, 0.03058293, 0.0234688 , 0.02184187],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6e0Sxa1d4GD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}